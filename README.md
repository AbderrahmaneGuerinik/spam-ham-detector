# ğŸ§  Spam/Ham Message Detector â€“ ML & NLP techniques

This project is a simple web interface to classify messages as spam or ham using ML and NLP techniques.

--- 
## ğŸ“¸ Screenshot

![App Screenshot](screenshot.png)

---

## ğŸš€ Live Demo  
ğŸ‘‰ [Live App on Render](https://spam-detector-abtc.onrender.com/) 

---

## ğŸ“¦ Features

- Real-time spam detection from text input
- Simple and interactive Streamlit interface

---

## âš™ï¸ Tech Stack

| Component     | Description                                  |
|---------------|----------------------------------------------|
| Backend       | Python, Scikit-learn, Transformers, SpaCy    |
| Frontend      | Streamlit                                    |
| NLP Tools     | `nltk`, `spacy`, `transformers`              |
| ML Model      | Xgboost, Naive bayes, Random forest                        |
| Deployment    | Docker, Render.com                           |

---

## ğŸ“š What I Learned

Throughout this project, I had the opportunity to reinforce and apply several key concepts in machine learning and natural language processing:

- ğŸ” **Revisited classification algorithms**: XGBoost, Naive Bayes, and Random Forest.
- ğŸ—ï¸ **Feature engineering**: Created new custom features to enhance model performance.
- ğŸ§¼ **Text preprocessing techniques**: Tokenization, stopword removal, punctuation stripping, etc.
- ğŸ§  **Text embeddings**: Used Word2Vec with Skip-Gram architecture to represent text semantically.
- ğŸ“Š **Model evaluation**: Kfold validation, evaluation metrics: Accuracy, F1-score, Precision, Recall, and Confusion Matrix.
- ğŸ—ƒï¸ **Model management**: Trained, saved, and reused ML models efficiently.
- ğŸŒ **App deployment**: Built and deployed a fully working Streamlit web app using Docker and Render.
- ğŸ“ˆ **Visualization**: Barcharts and wordcloud.

---

## ğŸ§ª Model Performance

After evaluating several classifiers, the XGBoost model delivered the best results:

- ğŸ¯ **XGBoost Accuracy**: 99%
- Other models tested: Naive Bayes, Random Forest
- Evaluated using metrics like **Precision**, **Recall**, **F1-Score**, and **Confusion Matrix**

--- 



